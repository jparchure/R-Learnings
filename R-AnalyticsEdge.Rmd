---
title: "Analytics Edge- Takeaway Notes"
author: "Jay Parchur√©"
date: "Started June 1, 2016"
output: html_document

---

### Linear Regression ### 
```
lm(DV ~ IV, data)
```
* **Baseline Model**- A model with no variables, usually set as a measure of central tendency (Mean, median)

* **SSE**- Square Sum of Errors

* **SST**- Square Sum of Errors from Baseline Model

* **R<sup>2</sup>** - is the error difference from the Baseline model. <br>
Formula: $$R^{2} = 1 - \frac{SSE}{SST}$$
<h4>Warning - SSE is not square root </h4>
--------------------------------

<!--#Problems
#Hard to cmpare 2 models
#Good models for easy problems will
#have an R squared close to 1.
#But good models for hard problems
#can still have an R squared close to zero.
#Throughout this course we will see
#`examples of both types of problems.

Make sure the predictor values are not correlated to each other

Iteratively simplify the model by removing non significant variable

Predictions- predict(model, testdata)
See summary of output, compare R2 between test and training data

-->

### Logistic Regression ###
```
glm(DV ~ IV, family=binomial)
```
* **Baseline Model**- In this case the most frequent outcome is chosen as the standard baseline method.

* **AIC**- Used to compare Models, but only on same data. The lower the better

* **Predictions**: The following formula returns probabilities of outcomes
```
predict(model, data, type="response")

#Simple Baseline Model
tapply(predictTrain, qualityTrain$PoorCare, mean)
``` 
Predictions can be calculated using a threshold value

### Outcome Measures ###

* **Confusion Matrix**- Table comparing Actual and Predicted Outcomes.
```
table(actualValues, predictedValues)
```
* All Rates are calculated with respect to **Actual Outcomes**

* **Sensitivity**- **Catch All the thieves** <br>
$$\frac{TP}{TP+FN}$$ 
  

* **Specificity**-  **Prevent all false alarms** <br>
$$\frac{TN}{TN+FP}$$

* **Accuracy**-
$$\frac{TN + TP}{N}$$

* **Error Rate**-
$$\frac{FN+FP}{TN}$$

* **False Negative Error Rate**-
$$\frac{FN}{TP+FN}$$

* **False Positive Error Rate**-
$$\frac{FP}{TP+FN}$$

* **ROC Curves**- Used when one type of error is more crucial than other types

### Missing Values ###

* Can be discovered using the ``` summary ``` function
* Some approaches involve removing the rows or columns with missing values, but this way you may lose data
* The <a href="https://cran.r-project.org/web/packages/mice/mice.pdf">MICE</a> package is very helpful for multiple imputation, which is a way of filling NA values.
* Make sure to run multiple imputation on all fields that are affected by missing values. This may include columns that are complete in themselves but are related to other imperfect ones.

</br>


